---
title: "TopicModeling"
author: "Ryan Cordell and Jonathan Fitzgerald"
date: "03/02/2017"
output: html_document
---

Last week we talked about modeling, and today we'll take on one method for modeling texts that has been extremely popular in recent digital humanities research: topic modeling or, if you want to impress someone, latent dirichlet allocation. Topic modeling is a technique that tends to work best on really *long* stretches of many *distinct* texts. In other words, we definitely need a corpus, and we can use the Wright American fiction collection from last week. 


# Modeling Topics with Mallet

Now we're going to use the `mallet` library to do some topic modelling. First, let's load the libraries we've been using: the same, now plus `mallet` and a new, strange one. Anyone know what `source("readWright.R")` is doing here?

```{r}

source("readWright.R")

```

Now we've got the Wright data just like last week. You might notice that we've got a more human readable title for these texts this time. If you look in the `readWright.R` file you'll see these two lines, which may by this point be self explanatory, though I'll confess that Fitz figured out the positive lookbehind at the front and positive lookahead at the end that bent my brain a little bit. There's [a breakdown here](https://regex101.com/r/q5pKwl/1).

```
WRIGHTregex <- regexpr("(?<=----\\s)([A-Z].*)(?=\\s\\.\\sDigital)", WRIGHT$text, perl = TRUE)
allWRIGHTtext$title <- regmatches(allWRIGHTtext$text, WRIGHTregex)
```

# Don't Forget Ngrams!!

Last week Fitz gave you a great introduction to text analysis in R, which starts with tokenizing. Before we delve into topic modeling, I wanted to briefly remind you that words are not the only possible tokens

```{r}

allWRIGHTtext %>%
  unnest_tokens(ngram,text,token = "ngrams", n = 5) %>% 
  group_by(title, ngram) %>% 
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  View()

```

Maybe it would be more interesting, however, to look at shared language. What does the code below do?

```{r}

WRIGHTgrams <- allWRIGHTtext %>%
  unnest_tokens(ngram,text,token = "ngrams", n = 4) %>% 
  group_by(title, ngram) %>% 
  summarize(count = n()) %>%
  ungroup() %>%
  group_by(ngram) %>%
  filter(n() > 1) %>%
  arrange(desc(ngram))
  
```

Ok, with that reminder in place let's move on to our main event today...

# Topic Models

The code below will prepare and build the model. This will likely be the most opaque code we've run in our class. We will discuss some of the details today, while for others you may need to refer directly to Blei, Wallach and Mimno's papers about the topic modeling algorithm. The primary bits of this code that you will want to change as you move forward are few: likely only the input data (in this case `allWRIGHTtext`) and `num.topics`, which determines...you guessed it...how many topics Mallet will sort the words in the corpus into.

```{r}
mallet.instances <- mallet.import(id.array = allWRIGHTtext$title, 
                                  text.array = allWRIGHTtext$text, 
                                  stoplist.file = "./data/stop_words.txt")

n.topics <- 25

topic.model <- MalletLDA(num.topics=n.topics, alpha.sum = 1, beta = 0.1)
topic.model$loadDocuments(mallet.instances)
topic.model$setAlphaOptimization(20, 50)
topic.model$train(500)
topic.model$maximize(10)
```

Now you can look at the most common words at greater length.

```{r}
# What are the top words in topics 2, 3, and 4?
topic.words <- mallet.topic.words(topic.model)

mallet.top.words(topic.model, word.weights = topic.words[2,], num.top.words = 10)
mallet.top.words(topic.model, word.weights = topic.words[3,], num.top.words = 10)
mallet.top.words(topic.model, word.weights = topic.words[4,], num.top.words = 10)
```

And we can use the top words in the topic to make a human readable "label" for each one. Remember, though, that the label does not comprise the topic; it's just a subset.

```{r}
topic_labels <- rep("", n.topics)
for (topic in 1:n.topics) {
  topic_labels[topic] <- paste(
    mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=" "
)}

```

When we model our topics, Mallet finds a number of other useful stats about our corpus. What are the lines below doing?
```{r}
vocabulary <- topic.model$getVocabulary()
word_freqs <- mallet.word.freqs(topic.model)
```
Try typing `length(vocabulary)` into the console. What does that tell you?




We can also correlate topics with texts:

```{r}

#Gets a list of the documents and topics
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
rownames(doc.topics) = allWRIGHTtext$title
colnames(doc.topics) = topic_labels

topicDF <- doc.topics %>%
  as_data_frame() %>%
  mutate(title = rownames(doc.topics)) %>%
  gather(topic, proportion, -title)
View(topicDF)

# we could also arrange our table by title
topicDF %>% 
  arrange(title) %>%
  View()

```

Now, there's something important to note before we go much farther. If you reran all of the topic modeling code above *without changing anything*, the topics derived would be similar, but *not identical* and *not listed in the same order*. This is because Mallet starts building the model using a random seed, meaning that it will not come to precisely the same conclusions in two subsequent analyses, even if all the parameters remain exactly the same. Let's talk a bit about the epistemelogical assumptions and consequences of that reality before we move on.

# Visualizing Topic Models

```{r}

ggplot(topicDF) + geom_tile(aes(x=title,y=topic,fill=proportion))

```

```{r}

ggplot(topicDF) + 
  geom_point(aes(x=topic,y=proportion)) + 
  coord_flip()

```

```{r}

ggplot(topicDF %>% filter(title == "Appell, Theron B.. The Knight of Castile" & proportion >= 0.1)) +
  geom_point(aes(x=topic,y=proportion)) + 
  coord_flip()

```

```{r}

ggplot(topicDF %>%
         filter(topic == "lank lady antoine") %>% 
         mutate(title = substr(title, 1,20)) %>%
         filter(proportion >= 0.1)) +
  geom_point(aes(x=title,y=proportion)) +
  labs(x="Story",y="Frequency of Topic 'lank lady antoine'") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

From [*The Historian's Macroscope*](http://www.themacroscope.org/?page_id=822), another way to visualize topics, which clusters them based on the similarity of words in each topic. You'll probably have to expand your plots window to really see this one:

```{r}

plot(hclust(dist(topic.words)), labels=topic_labels)

```

Finally, some visualization candy from Fitz. You'll need to install the `plotly` package. The resulting plot might look familiar, but hover your mouse over it to see why it's something far more interesting than we produced above.

```{r}

p = topicDF %>%
  group_by(topic) %>%
  mutate(id = 1:n()) %>% 
  ggplot() + 
  geom_tile(aes(x=id,y=topic,fill=proportion,text = paste("title:", title)))

library(plotly)
ggplotly(p)

```

## Exercises

This week the exercise is simple: you need to produce and graph topic models based on a corpus of your choosing. If your corpus is mostly metadata, this may require finding some more text heavy data for this week's exercises. 