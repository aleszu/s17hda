---
title: "Exploratory Data Analysis"
author: "Ryan Cordell"
date: "1/31/2017"
output: html_document
---

```{r}
library(tidyverse)
```

# The Pipe Operator

Last week we explictly invoked each function in a new line of code. Today we'll introduce the pipe operator `%>%`, which allows us to chain together a series of transformations. Let's illustrate this with a few familiar blocks from last week:

```{r}
census <- read.csv(file = "./data/1840-census-data.csv")
census <- select(census, 1,6:81)
census <- rename(census, county = QualifyingAreaName)
census_long <- gather(census, "identification", "count", 2:77)
census_long <- separate(census_long, county, into = c("county", "state"), sep = "\\, ")
census_long <- na.omit(census_long)
```

Using pipes, we can chain together these operations like so, to create two variables:

```{r}
census <- read.csv(file = "./data/1840-census-data.csv") %>%
  select(1,6:81) %>%
  rename(county = QualifyingAreaName)

census_long <- census %>%
  gather("identification", "count", 2:77) %>%
  separate(county, into = c("county", "state"), sep = "\\, ") %>%
  na.omit(census_long)
```

Or like so, to create only one `census_long` variable (note that you'll have to clear your `Global Environment` before running this code to see it actually work.)

```{r}
census_long <- read.csv(file = "./data/1840-census-data.csv") %>%
  select(1,6:81) %>%
  rename(county = QualifyingAreaName) %>%
  gather("identification", "count", 2:77) %>%
  separate(county, into = c("county", "state"), sep = "\\, ") %>%
  na.omit(census_long)
```

What did each of these do? We'll talk about that together.

Note that there are some distinct structural differences when using pipes. For one, the variable being transformed is usually invoked at the beginning of the chain, and thus does not appear as an argument in the separate parts of the chain. Compare these piped operations with the line-by-line operations above again. Where do you see `census` or `census_long` invoked in lines 17-22 when they are not in lines 41-46? 

We can also use pipes to make (and view) temporary transformations in our data that won't be saved as variables. This is a very useful way of seeing what a series of operations will do before "really" running them.

```{r}
census_long %>% 
  spread(identification, count) %>%
  View()
```

# Transforming Data Frames

So what kinds of transformations might we want to make with dataframes? To begin thinking through that question, let's import a small sample of data from the Viral Texts Project. In the code box below, import the CSV `VT-vignettes.csv` from the class data folder:

```{r}


```

There are *lots* of quirks in this data, which we'll talk through together. I want to use this data to talk about a few more features of dataframes in R and . First you'll notice that unlike our census data from last week, the different columns here comprise many different data types: text, numbers, dates, etc. Data types are important and sometimes frustrating in R; there are functions that can operate on strings (text data) but not other kinds. You can determine the data type of anything in R using the `class()` function:

```{r}

# you can investigate the class of a whole variable:
class(vtClusters)
# or of a variable within a dataframe
class(vtClusters$text)
class(vtClusters$date)
```

Notice anything about the class of our text and date column? By default, when creating dataframes R converts textual data—including numbers with typographical features, like the hyphens in our date column—to `factors`. One of the most human readable definitions of factors can be found at [https://www.r-bloggers.com/data-types-part-3-factors/](https://www.r-bloggers.com/data-types-part-3-factors/). In short, however, factors are wonderful for variables you might want summary statistics about, but not always wonderful for other data types. If we wanted to work with dates *qua* dates—to find out which reprints occurred earlier than others—a factor `date` column might not serve. Likewise for doing certain kinds of text analysis on the `text` column. 

When importing tabular data we can include an argument that will `coerce` (read: force) strings to import as strings rather than factors. There are also functions in R for coercing one data type into another: `as.Date`,  `as.character`, `as.dataframe`, and so forth. Can you identify the two coercions in the code below?

```{r}

vtClusters <- read.csv(file="../s17hda-github/data/VT-vignettes.csv", stringsAsFactors=FALSE)
vtClusters$date <- as.Date(vtClusters$date, "%Y-%m-%d")

```

Now check the class of the `text` and `date` columns in the console. Also type `vtClusters` into the console to see what prints.

## Tibbles

Most recently, lots of folks have moved away from base R's dataframes and to "tibbles", which are essentially a new protocol for creating dataframes developed by Hadley Wickham, of Tidyverse fame. You can read more about tibbles using the command `vignette("tibble")` in the console. In most ways they act just like dataframes, and indeed they are a subspecies of dataframe, but they correct a few annoyances of the dataframe. One big one: when importing data into a tibble the data types and column names will not be converted, so you need not convert strings to factors. Other fields, such as dates, might still need to be coerced. 

To convert data into a tibble, you can use the functions `as_data_frame` (with underscores, not periods) or `as_tibble`. You can either convert an existing dataframe or do this upon importing new data. Below we're going to use pipes to import our Viral Texts data as a tibble, coerce the date column into the date class, and do one other thing. Can you tell what that is?

```{r}

vtClusters <- as_tibble(read.csv(file="../s17hda-github/data/VT-vignettes.csv")) %>% 
  mutate(title = gsub('(.*)\\.*\\(.*','\\1',title)) %>%
  mutate(date = as.Date(date, "%Y-%m-%d"))

```

Now type `vtClusters` into the console again to see what prints. Notice the difference? The print behavior of tibbles is far less loquacious than standard data frames. Before we move on, can you write some code into the code block above so that our final `vtClusters` dataframe includes **only** the `cluster`, `date`, and `title` columns?

## From Bibliography to Network

One of the transformations I most often need for Viral Texts brings us from a list of reprints organized in "clusters"—essentially ennumerative bibliographies—to data that expresses the network relationships among newspapers within those clusters. Essentially I want to reorganize the table so that co-membership in a given cluster creates a line of "edge" data between all the newspapers within that cluster. If that doesn't make sense, run the code below and we'll talk through what it does and the transformation it produces. 

```{r}

vtClusters <- as_data_frame(read.csv(file="../s17hda-github/data/VT-vignettes.csv")[ , c("cluster", "date", "title")]) %>% 
  mutate(title = gsub('(.*)\\.*\\(.*','\\1',title)) %>%
  mutate(date = as.Date(date, "%Y-%m-%d")) %>%
  full_join(vtClusters, vtClusters, by = "cluster") %>%
  filter(date.x < date.y) %>%
  View()

```

We could also structure things to put the new edges table into a new variable:

```{r}

vtClusters <- as_data_frame(read.csv(file="../s17hda-github/data/VT-vignettes.csv")[ , c("cluster", "date", "title")]) %>% 
  mutate(title = gsub('(.*)\\.*\\(.*','\\1',title)) %>%
  mutate(date = as.Date(date, "%Y-%m-%d"))

vtDirected <- full_join(vtClusters, vtClusters, by = "cluster") %>%
  filter(date.x < date.y)
View(vtDirected)

```

So now we have a dataframe expressing every possible edge derived from the cluster data provided. From here we might decide to add even more nuance. In most network graphs, every observed relationship between two entities is assigned a weight of 1, and when two entities are connected multiple times (in this case, when two newspapers are members of multiple clusters) the overall weight of their relationship increases by 1 for each observation. 

Recently, however, I've been experimenting with other ways of weighting relationships based on features in my reprinting data. For instance, how might we adjust our weights based on the time lag between two observed reprintings of the same text? The code below is one experiment I've made along those lines; we can talk through my logic once you run it. 

```{r}

vtEdges <- vtDirected %>%
  mutate(lag = date.y - date.x) %>%
  mutate(lagWeight = 1 / as.numeric(lag)) %>%
  group_by(title.x,title.y) %>%
  summarise(lag =mean(lag), weight = sum(lagWeight), rawWeight = n()) %>%
  mutate(lagEffect = weight - rawWeight) %>%
  arrange(desc(weight)) %>%
  rename(source = title.x, target = title.y)

View(vtEdges)

```

Before we move on, I realize that we haven't yet discussed how to output data *from* Rstudio. Like importing data, there are different functions for exporting different data formats. If we wanted to export a CSV of our network data—to import into network software like Gephi, say—we could do the following:

```{r}

write.csv(vtEdges, file="./output/vtEdges.csv")

```
