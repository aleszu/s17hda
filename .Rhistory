newspaperPages %>%
unnest_tokens(word,text,token = "ngrams", n=5) %>%
group_by(title,word) %>%
summarize(count=n()) %>%
spread(title,count,fill=0) %>%
# filter(LewisburgChronicle >= 1 & VermontPhoenix >= 1) %>%
View()
sharedFiveGrams <- newspaperPages %>%
unnest_tokens(word,text,token = "ngrams", n=5) %>%
group_by(title,word) %>%
summarize(count=n()) %>%
spread(title,count,fill=0) %>%
filter(LewisburgChronicle >= 1 & VermontPhoenix >= 1) %>%
rename(fivegram = word)
sharedFiveGrams$sharedSum = sharedFiveGrams$LewisburgChronicle + sharedFiveGrams$VermontPhoenix
sharedFiveGrams <- arrange(sharedFiveGrams, desc(sharedSum))
sharedFiveGrams <- newspaperPages %>%
unnest_tokens(word,text,token = "ngrams", n=5) %>%
group_by(title,word) %>%
summarize(count=n()) %>%
spread(title,count,fill=0) %>%
filter(LewisburgChronicle >= 1 & VermontPhoenix >= 1) %>%
rename(fivegram = word)
sharedFiveGrams$sharedSum = sharedFiveGrams$LewisburgChronicle + sharedFiveGrams$VermontPhoenix
sharedFiveGrams <- arrange(sharedFiveGrams, desc(sharedSum))
View(sharedFiveGrams)
sharedFiveGrams %>%
filter(sharedSum > 2) %>%
ggplot() +
geom_bar(stat="identity") +
aes(x=fivegram,y=sharedSum) +
labs(title = "Shared Five Grams Between Newspaper Issues", y = "Total Number Shared", x = "Five Grams Shared")
brown <- tibble(text=read_file("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt"))
library(tidyverse)
brown <- tibble(text=read_file("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt"))
brown_quotes <- brown %>%
gsub([ -]("[A-Za-z].*?")[ -])
brown_quotes <- brown %>%
gsub([ -]"[A-Za-z].*?"[ -])
brown_quotes <- brown %>%
gsub("[ -]"[A-Za-z].*?"[ -]"")
brown_quotes <- brown %>%
grep("[ -]"[A-Za-z].*?"[ -]"")
brown_quotes <- brown %>%
grep([ -]"[A-Za-z].*?"[ -]")
brown_quotes <- brown %>%
grep([\s-]"[A-Za-z].*?"[\s-]")
brown_quotes <- brown %>%
grep([\s|-]"[A-Za-z].*?"[\s|-]")
brown_quotes <- grep("[\s-]\"[A-Za-z].*?\"[\s-]",brown)
brown_quotes <- grep("[ -]\"[A-Za-z].*?\"[ -]",brown)
brown_quotes
brown_quotes <- grep("[\\s-]\"[A-Za-z].*?\"[\\s-]",brown,value=T)
brown_quotes
brown <- tibble(text=read_file("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt")) %>%
paste(unlist(brown)), collapse=" ")
brown <- paste(unlist(brown)), collapse=" ")
brown <- paste(unlist(brown), collapse=" ")
brown_quotes <- grep("[\\s-]\"[A-Za-z].*?\"[\\s-]",brown,value=T)
brown_quotes <- brown %>% gsub('(.*?)(\".*?\")','\\2',.)
brown_quotes
brown <- tibble(text=read_file("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt",sep="\n",what="raw",skip = 365))
brown <- scan("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt",sep="\n",what="raw",skip = 365))
brown <- scan("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt",sep="\n",what="raw",skip = 365)
brown <- paste(unlist(brown), collapse=" ")
brown_quotes <- brown %>% gsub('(.*?)(\".*?\")','\\2',.)
brown_quotes
brown <- tibble(text=read_file("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt",sep="\n",what="raw",skip = 365))
brown <- tibble(text=read_file("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt",sep="\n"))
brown <- tibble(text=read_file("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt")) %>%
paste(readLines, collapse = "\n")
brown <- tibble(text=read_file("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt"))
View(brown)
brown <- paste(readLines(text), collapse = "\n")
paste(readLines(text), collapse = "\n")
gsub("\r\n\","",text)
brown <- tibble(text=read_file("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt")) %>%
gsub("\r\n\","",text)
gsub("\r\n\","",brown$text)
brown <- tibble(text=read_file("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt"))
gsub("[\r\n\]", " ", text)
gsub("[\r\n]", " ", text)
gsub("[\r\n]", " ", text)
gsub("[\r\n]", " ", brown$text)
View(brown)
head(brown$text)
brown <- as_data_frame(text=read_file("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt"))
brown <- tibble(text=read_file("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt"))
brown <- paste(readLines("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt"), collapse = "\n")
brown_quotes <- brown %>% gsub('(.*?)(\".*?\")','\\2',.)
brown_quotes
brown_quotes <- grep("[ -](\"[A-Za-z].*?\")[ -]", brown)
brown_quotes <- regexpr("[ -](\"[A-Za-z].*?\")[ -]", brown)
brown_quotes
brown_quotes <- regmatches("[ -](\"[A-Za-z].*?\")[ -]", brown)
brown_quotes <- regexpr("[ -](\"[A-Za-z].*?\")[ -]", brown)
regmatches(brown,brown_quotes,invert=FALSE)
brown_quotes <- regmatches(brown,brown_quotes,invert=FALSE)
brown_quotes
brown <- paste(readLines("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt"), collapse = "\n")
brown_quotes <- brown %>% gsub('(.*?)(\".*?\")','\\2',.)
brown_quotes <- brown %>% gsub('(.*?)(\".*?\")','\\2',.)
brown_quotes <- regexpr("[ -](\"[A-Za-z].*?\")[ -]", brown)
brown_quotes <- as_data_frame(regmatches(brown,brown_quotes,invert=FALSE))
View(brown_quotes)
install.package("stringr")
install.packages("stringr")
library(stringr)
quotes <- "[ -]("[A-Za-z].*?")[ -]"
quotes <- "[ -](\"[A-Za-z].*?\")[ -]"
brown_quotes <- str_extract_all(brown, quotes)
brown_quotes
quotes <- "[\\s-](\"[A-Za-z].*?\")[\\s-]"
brown_quotes <- str_extract_all(brown, quotes)
View()
View(brown_quotes)
brown <- paste(readLines("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt"), collapse = "\n")
quotes <- "[\\s|-](\"[A-Za-z].*?\")[\\s|-]"
brown_quotes <- str_extract_all(brown, quotes)
View(brown_quotes)
quotes <- "[\\s|-]\"[A-Za-z].*?\"[\\s|-]"
brown_quotes <- str_extract_all(brown, quotes)
View(brown_quotes)
quotes <- "[\\s|-](\"[A-Za-z].+\")[\\s|-]"
brown_quotes <- str_extract_all(brown, quotes)
View(brown_quotes)
View(brown_quotes)
brown <- paste(readLines("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt"), collapse = "\n")
quotes <- "[\\s-](\"[A-Za-z].*?\")[\\s-]"
brown_quotes <- str_extract_all(brown, quotes)
View(brown_quotes)
View(brown)
brown <- paste(readLines("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt"))
View(brown)
brown <- paste(readLines("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt"))
View(brown)
brown <- paste(readLines("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt"), sep="\n")
View(brown)
brown <- tibble(text=read_file("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt"))
quotes <- "[\\s-](\"[A-Za-z].*?\")[\\s-]"
brown_quotes <- str_extract_all(brown$text, quotes)
View(brown_quotes)
quotes <- "(\"[A-Za-z].*?\")"
brown_quotes <- str_extract_all(brown$text, quotes)
View(brown_quotes)
View(brown)
View(brown$text)
View(brown$text)
brown <- gsub("\r\n", " ")
brown <- gsub("\r\n", " ", brown$text)
View(brown$text)
brown <- tibble(text=read_file("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt"))
brown$text <- gsub("\r\n", " ", brown$text)
View(brown$text)
quotes <- "(\"[A-Za-z].*?\")"
brown_quotes <- str_extract_all(brown$text, quotes)
View(brown_quotes)
install.packages("stringr")
library(stringr)
brown <- tibble(text=read_file("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt"))
brown$text <- gsub("\r\n", " ", brown$text)
gsub("([.!?])([A-Za-z]", "\1 \2"")
gsub("([.!?])([A-Za-z]", "\1 \2")
gsub("([.!?])([A-Za-z]", "\1 \2", brown$text)
gsub("([.!?])([A-Za-z])", "\1 \2", brown$text)
quotes <- "(\"[A-Za-z].*?\")"
brown_quotes <- str_extract_all(brown$text, quotes)
View(brown_quotes)
crewlist <- tibble(read.csv("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/crewlists.csv", header=TRUE, sep=","))
crewlist <- read.csv("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/crewlists.csv", header=TRUE, sep=","))
crewlist <- read.csv("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/crewlists.csv", header=TRUE, sep=",")
View(crewlist)
View(crewlist)
View(crewlist)
crewlist$ApproximateDeparture <- gsub("\d{1,2}\/\d{1,2}\/(\d{4})", "\1", crewlist$ApproximateDeparture)
crewlist$ApproximateDeparture <- gsub("\\d{1,2}\/\\d{1,2}\/(\\d{4})", "\1", crewlist$ApproximateDeparture)
crewlist$ApproximateDeparture <- gsub("\\d{1,2}[/]\d{1,2}[/](\\d{4})", "\1", crewlist$ApproximateDeparture)
crewlist$ApproximateDeparture <- gsub("[0-9]{1,2}[/][0-9]{1,2}[/]([0-9]{4})", "\1", crewlist$ApproximateDeparture)
crewlist <- read.csv("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/crewlists.csv", header=TRUE, sep=",")
crewlist$ApproximateDeparture <- gsub("[0-9]{1,2}\/[0-9]{1,2}\/([0-9]{4})", "\1", crewlist$ApproximateDeparture)
crewlist$ApproximateDeparture <- gsub("[0-9]{1,2}\\/[0-9]{1,2}\\/([0-9]{4})", "\1", crewlist$ApproximateDeparture)
crewlist <- read.csv("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/crewlists.csv", header=TRUE, sep=",")
crewlist$ApproximateDeparture <- gsub("[0-9]{1,2}\/[0-9]{1,2}\/([0-9]{4})", "\1", crewlist$ApproximateDeparture)
crewlist$ApproximateDeparture <- gsub("[0-9]{1,2}\\/[0-9]{1,2}\\/([0-9]{4})", "\\1", crewlist$ApproximateDeparture)
2+2
5^32
2+2+2
5^32
2+2
2+2
2+2
2+2
5^32
2+2
5^32
plot(1:100,(1:100)^2)
plot(1:100,(1:100)^2)
plot(1:100,(1:100)^2)
library(tidyverse)
library(tidyverse)
library(dplyr)
library(tidytext)
library(wordcloud)
raven <- data_frame(text=read_file("http://chroniclingamerica.loc.gov/lccn/sn85055199/1849-11-28/ed-1/seq-1/ocr.txt"))
View(raven)
View(raven)
raven <- tibble(text=read_file("http://chroniclingamerica.loc.gov/lccn/sn85055199/1849-11-28/ed-1/seq-1/ocr.txt")) # a tibble is special kind of dataframe we'll learn more about in week 3
raven %>%
unnest_tokens(word,text) %>%
count(word,sort=T)
raven %>%
unnest_tokens(word,text) %>%
count(word,sort=T) %>%
with(wordcloud(word,n,max.words = 100))
raven %>%
unnest_tokens(word,text) %>%
count(word,sort=T) %>%
with(wordcloud(word,n,max.words = 100))
data("stop_words") # <- Load the stop words data
raven %>%
unnest_tokens(word,text) %>%
count(word,sort=T) %>%
anti_join(stop_words) %>% #<- This is the only change to our little program from before.
with(wordcloud(word,n))
raven %>%
unnest_tokens(word,text,token="ngrams",n=5) %>%
count(word,sort=T)
raven %>%
unnest_tokens(word,text,token="ngrams",n=5) %>%
count(word,sort=T)
raven %>%
unnest_tokens(word,text,token="ngrams",n=3) %>%
group_by(word) %>%
filter(n()>3) %>%
ggplot() +
geom_bar(fill="red") +
aes(x=word) +
coord_flip()
ravenGrams <- as_data_frame(raven %>%
unnest_tokens(word,text,token="ngrams",n=5) %>%
count(word,sort=T))
View(ravenGrams)
head(ravenGrams)
tail(ravenGrams)
class(ravenGrams)
newspaperPages <- data_frame(
text = c(text=read_file("http://chroniclingamerica.loc.gov/lccn/sn85055199/1849-11-28/ed-1/seq-1/ocr.txt"),text=read_file("http://chroniclingamerica.loc.gov/lccn/sn98060050/1845-02-28/ed-1/seq-1/ocr.txt")),
title = c("LewisburgChronicle","VermontPhoenix"))
newspaperPages %>%
unnest_tokens(word,text,token = "ngrams", n = 5) %>% # <- New
group_by(title, word) %>%
summarize(count = n()) %>%
arrange(desc(count)) %>%
View()
newspaperPages %>%
unnest_tokens(word,text,token = "ngrams", n=5) %>%
group_by(title,word) %>%
summarize(count=n()) %>%
spread(title,count,fill=0) %>%
# filter(LewisburgChronicle + VermontPhoenix > 2) %>%
ggplot() +
aes(x=LewisburgChronicle,y=VermontPhoenix,label=word) +
geom_point(alpha=.3) +
geom_text(check_overlap = TRUE) +
#  scale_x_log10() +
#  scale_y_log10() +
geom_abline(color = "red")
newspaperPages %>%
unnest_tokens(word,text,token = "ngrams", n=5) %>%
group_by(title,word) %>%
summarize(count=n()) %>%
spread(title,count,fill=0) %>%
# filter(LewisburgChronicle >= 1 & VermontPhoenix >= 1) %>%
View()
sharedFiveGrams <- newspaperPages %>%
unnest_tokens(word,text,token = "ngrams", n=5) %>%
group_by(title,word) %>%
summarize(count=n()) %>%
spread(title,count,fill=0) %>%
filter(LewisburgChronicle >= 1 & VermontPhoenix >= 1) %>%
rename(fivegram = word)
sharedFiveGrams$sharedSum = sharedFiveGrams$LewisburgChronicle + sharedFiveGrams$VermontPhoenix
sharedFiveGrams <- arrange(sharedFiveGrams, desc(sharedSum))
View(sharedFiveGrams)
crewlist <- read.csv("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/crewlists.csv", header=TRUE, sep=",")
View(crewlist)
crewlist$ApproximateDeparture <- gsub("[0-9]{1,2}\\/[0-9]{1,2}\\/([0-9]{4})", "\\1", crewlist$ApproximateDeparture)
brown <- tibble(text=read_file("https://raw.githubusercontent.com/jonathandfitzgerald/s17hda/master/data/NarrativeWilliamWellsBrown.txt"))
brown$text <- gsub("\r\n", " ", brown$text) %>%
gsub("([.!?])([A-Za-z])", "\1 \2", brown$text)
quotes <- "(\"[A-Za-z].*?\")"
brown_quotes <- str_extract_all(brown$text, quotes)
View(brown_quotes)
sharedFiveGrams %>%
filter(sharedSum > 2) %>%
ggplot() +
geom_bar(stat="identity") +
library(stringr)
sharedFiveGrams %>%
filter(sharedSum > 2) %>%
install.packages(c("BH", "chron", "cluster", "codetools", "colorspace", "curl", "data.table", "digest", "foreign", "ggplot2", "git2r", "hms", "hunspell", "janeaustenr", "jsonlite", "knitr", "lattice", "Matrix", "mgcv", "mnormt", "nlme", "openssl", "psych", "Rcpp", "rmarkdown", "scales", "selectr", "sentimentr", "slam", "survival", "tidyr", "xml2", "yaml", "zoo"))
2+2
5^32
2+2
5^32
library(dplyr)
library(tidytext)
library(wordcloud)
raven <- data_frame(text=read_file("http://chroniclingamerica.loc.gov/lccn/sn85055199/1849-11-28/ed-1/seq-1/ocr.txt"))
library(dplyr)
library(tidytext)
library(wordcloud)
??length
??dplyr
library(tidyverse)
??gather
setwd("Dropbox/Teaching/HDA/s17hda-github/")
getwd
getwd()
?tidyr
??tidyr
??spread
??setwd
?setwd
?spread
help(spread)
2+2
5^32
2+2
5^32
bartleby <- paste(readLines(http://www.gutenberg.org/cache/epub/11231/pg11231.txt), collapse = "\n")
bartleby <- paste(readLines(http://www.gutenberg.org/cache/epub/11231/pg11231.txt), collapse = "\n")
bartleby <- paste(readLines("http://www.gutenberg.org/cache/epub/11231/pg11231.txt""), collapse = "\n")
bartleby <- paste(readLines("http://www.gutenberg.org/cache/epub/11231/pg11231.txt""))
bartleby <- paste(readLines("http://www.gutenberg.org/cache/epub/11231/pg11231.txt""))
bartleby <- paste(readLines("http://www.gutenberg.org/cache/epub/11231/pg11231.txt"), collapse = "\n")
head(bartleby)
tail(bartleby)
words <- tokenize_words(bartleby)
length(words[[1]])
words <- tokenize_words(bartleby)
length(words[[1]])
install.packages("tokenizers")
install.packages("tokenizers")
summary(bartleby)
library(tidyverse)
library(tokenizers)
words <- tokenize_words(bartleby)
length(words[[1]])
?gather
setwd("../s17hda/")
setwd("../s17hda-github/")
getwd()
getwd()
:
setwd("~/Dropbox/Teaching/HDA/s17hda-github/data/")
setwd("~/Dropbox/Teaching/HDA/s17hda-github/")
clusters <- read.csv(file = "../data/VT-vignettes.csv")
clusters <- read.csv(file = "./data/VT-vignettes.csv")
View(clusters)
type(clusters$date)
class(clusters$date)
class(clusters)
summary(clusters)
summary(clusters$cluster)
clusters$text
type(clusters)
class(clusters)
summary(clusters$date)
summary(clusters$issue)
summary(clusters$title)
summary(clusters$title)
library(tidyverse)
getwd()
census <- read.csv(file = "./data/1840-census-data.csv")
summary(census$Newspapers)
summary(census$FreeColoredPopulation)
View(census)
summary(census)
str(census)
?select
census <- select(census, Newspapers, Newspapers_Daily, Newspapers_Weekly, Newspapers_SemiTriWeekly, Periodicals, PrintingOffices, Binderies, NumberofPersonsEmployedinPrintingBinding)
census <- read.csv(file="./data/1840-census-data.csv")[ , c("Newspapers", "Newspapers_Daily", "Newspapers_Weekly", "Newspapers_SemiTriWeekly", "Periodicals", "PrintingOffices", "Binderies", "NumberofPersonsEmployedinPrintingBinding")]
census <- read.csv(file="./data/1840-census-data.csv")[ , c("Newspapers", "Newspapers_Daily", "Newspapers_Weekly", "Newspapers_SemiTriWeekly", "Periodicals", "PrintingOffices")]
census <- select(census, QualifyingAreaName, Newspapers, Newspapers_Daily, Newspapers_Weekly, Newspapers_SemiTriWeekly, Periodicals, PrintingOffices, Binderies, NumberofPersonsEmployedinPrintingBinding)
census <- read.csv(file = "./data/1840-census-data.csv")
View(census)
census <- select(census, QualifyingAreaName, Newspapers, Newspapers_Daily, Newspapers_Weekly, Newspapers_SemiTriWeekly, Periodicals, PrintingOffices, Binderies, NumberofPersonsEmployedinPrintingBinding)
census <- read.csv(file="./data/1840-census-data.csv")[ , c("QualifyingAreaName", "Newspapers", "Newspapers_Daily", "Newspapers_Weekly", "Newspapers_SemiTriWeekly", "Periodicals", "PrintingOffices")]
rename(census, QualifyingAreaName = County)
rename(census, County = QualifyingAreaName)
rename(census, County = QualifyingAreaName)
rename(census, county = QualifyingAreaName)
rename(census, county = QualifyingAreaName)
rename(census, county = QualifyingAreaName)
census <- read.csv(file="./data/1840-census-data.csv")[ , c("QualifyingAreaName", "Newspapers", "Newspapers_Daily", "Newspapers_Weekly", "Newspapers_SemiTriWeekly", "Periodicals", "PrintingOffices")]
rename(census, county=QualifyingAreaName)
View(census)
census <- rename(census, county = QualifyingAreaName)
View(census)
printCenters <- filter(census, PrintingOffices >= 5)
View(printCenters)
unique(census, Newspapers)
distinct(census, Newspapers)
census <- mutate(census, serials = Newspapers + Periodicals)
head(census$serials)
View(census)
census <- arrange(census, serials, newspapers, periodicals)
census <- arrange(census, serials, Newspapers, Periodicals)
View(census)
census <- arrange(census, serials, Newspapers, Periodicals)
census <- arrange(census, desc(serials, Newspapers, Periodicals))
census <- arrange(census, serials, Newspapers, Periodicals)
census <- read.csv(file="./data/VT.vignettes.csv")[ , c("cluster", "date", "title")]
census <- read.csv(file="./data/VT-vignettes.csv")[ , c("cluster", "date", "title")]
census <- read.csv(file="./data/1840-census-data.csv")[ , c("QualifyingAreaName", "Newspapers", "Newspapers_Daily", "Newspapers_Weekly", "Newspapers_SemiTriWeekly", "Periodicals", "PrintingOffices")]
vignettes <- read.csv(file="./data/VT-vignettes.csv")[ , c("cluster", "date", "title")]
View(vignettes)
vignettes <- spread(vignettes, title, date)
crewlist <- read.csv(file="./data/crewlists.csv")
View(crewlist)
View(census)
census <- rename(census, county = QualifyingAreaName)
census <- mutate(census, serials = Newspapers + Periodicals)
head(census$serials)
census <- arrange(census, serials, Newspapers, Periodicals)
View(census)
census <- separate(census, county, into = c("county", "state"), sep = "\\, ")
View(census)
View(census[1259,])
census[1259,]
census[1259,1:2]
census <- read.csv(file = "./data/1840-census-data.csv")
census <- select(census, 1,5:81)
View(census)
census <- select(census, 1,6:81)
census <- select(census, 1, 6:81)
census <- read.csv(file = "./data/1840-census-data.csv")
census <- select(census, 1,6:81)
census <- read.csv(file = "./data/1840-census-data.csv")
census <- select(census, 1,6:81)
census <- rename(census, county = QualifyingAreaName)
census <- separate(census, county, into = c("county", "state"), sep = "\\, cen")
View(census)
census <- read.csv(file = "./data/1840-census-data.csv")
census <- select(census, 1,6:81)
census <- rename(census, county = QualifyingAreaName)
census <- separate(census, county, into = c("county", "state"), sep = "\\, cen")
census <- read.csv(file = "./data/1840-census-data.csv")
census <- select(census, 1,6:81)
census <- rename(census, county = QualifyingAreaName)
View(census)
census_long <- gather(census, "identification", "count", 2:77)
View(census_long)
census <- separate(census, county, into = c("county", "state"), sep = "\\, ")
census <- read.csv(file = "./data/1840-census-data.csv")
census <- select(census, 1,6:81)
census <- rename(census, county = QualifyingAreaName)
census_long <- gather(census, "identification", "count", 2:77)
census_long <- separate(census_long, county, into = c("county", "state"), sep = "\\, ")
census_long <- group_by(census_long, identification)
summarize(census_long, count = n())
census_ids <- summarize(census_long, count = n())
View(census_ids)
census_ids <- summarize(census_long, sum(count))
View(census_ids)
census_ids <- summarize(census_long, sum(census_long$count))
View(census_ids)
census_ids <- summarize(census_long, total_ids = sum(count))
View(census_ids)
census_ids <- summarize(census_long, total_ids = sum(census_long$count))
View(census_ids)
census_ids <- summarize(census_long, identification, total_ids = sum(census_long$count))
census_ids <- summarize(census_long)
View(census_ids)
census_ids <- summarize(census_long, sum(count))
View(census_ids)
census_ids <- summarize(census_long, sum(census_long$count))
View(census_ids)
census_ids <- summarize(census_long, total_id = sum(census_long$count))
View(census_ids)
census_long %>%
group_by(identification) %>%
summarize(total_ids = n())
census_ids <- census_long %>%
group_by(identification) %>%
summarize(total_ids = n())
View(census_ids)
census_ids <- census_long %>%
group_by(identification) %>%
summarize(total_ids = sum(count))
View(census_ids)
type(census_long$count)
census_long <- na.omit(census_long)
census <- read.csv(file = "./data/1840-census-data.csv")
census <- select(census, 1,6:81)
census <- rename(census, county = QualifyingAreaName)
census_long <- gather(census, "identification", "count", 2:77)
census_long <- separate(census_long, county, into = c("county", "state"), sep = "\\, ")
census_long <- na.omit(census_long)
census_ids <- census_long %>%
group_by(identification) %>%
summarize(total_ids = sum(count))
View(census_ids)
census_long <- group_by(census_long, identification)
census_ids <- summarize(census_long, total_ids = sum(count))
View(census_ids)
census_long <- group_by(census_long, state, identification)
census_ids <- summarize(census_long, total_ids = sum(count))
View(census_ids)
census_wide <- spread(census_long, identification, count)
View(census_wide)
